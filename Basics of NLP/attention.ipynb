{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51781072",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5fbe91f",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d00545",
   "metadata": {},
   "source": [
    "In NLP (Natural Language Processing), attention is a mechanism that allows models to focus on specific parts of the input sequence when processing or generating output, rather than treating all parts equally. It helps the model weigh the importance of different words or tokens based on their relevance to the task, such as translation, summarization, or question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed8dff7",
   "metadata": {},
   "source": [
    "### Key Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380265ef",
   "metadata": {},
   "source": [
    "How it Works: Attention computes a weighted sum of input representations, where the weights are determined by a compatibility score (e.g., dot product, scaled dot-product) between the current item and all other items in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef67bb17",
   "metadata": {},
   "source": [
    "### Types:\n",
    "Self-Attention: Each token attends to all other tokens in the same sequence (used in Transformers).\n",
    "Cross-Attention: A token in one sequence (e.g., target) attends to tokens in another sequence (e.g., source), common in encoder-decoder models.\n",
    "\n",
    "Benefit: Enables the model to capture long-range dependencies and context more effectively than traditional RNNs or CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71cd582",
   "metadata": {},
   "source": [
    "### Example\n",
    "In machine translation (\"The cat sat on the mat\" → \"El gato se sentó en la alfombra\"):\n",
    "\n",
    "When translating \"sat,\" the model uses attention to focus more on \"sat\" and \"on\" in the source sentence, rather than equally on all words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467fd4be",
   "metadata": {},
   "source": [
    "### Impact\n",
    "Attention is the foundation of Transformer models (e.g., BERT, GPT), making them highly efficient and scalable for various NLP tasks.\n",
    "\n",
    "For more details, you can explore the original Transformer paper or recent implementations on the web or X posts about NLP advancements."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
